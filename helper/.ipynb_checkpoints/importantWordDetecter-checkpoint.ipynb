{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset \n",
    "2. pick 50 Documents for now\n",
    "3. make sentence splitting\n",
    "4. drop each word after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonline(filename, limit):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            counter += 1\n",
    "            py_obj = json.loads(line)\n",
    "            data.append(py_obj)\n",
    "            if counter > limit:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick first 60 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_jsonline('../data/items_reviews_18.jl', 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_id': 3321611, 'source_id': 277605655, 'title': 'This place is incredible!', 'text': 'I visited this b&b during a short trip to ride the famous Belgian pavé and it was perfect. The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle! It is in a really good location for riding or driving into Oudenaarde (approx 10 mins) and there are some brilliant restaurants close by. I cannot recommend this place enough!', 'user_rating': 5, 'lang': 'en', '_type': 'TripAdvisorHotelReviewItem'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Splitting\n",
    "- have list with text items\n",
    "- have list with splitted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Text form Reviews\n",
    "sentences = []\n",
    "for obj in data:\n",
    "    sentences.append(obj[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I visited this b&b during a short trip to ride the famous Belgian pavé and it was perfect. The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle! It is in a really good location for riding or driving into Oudenaarde (approx 10 mins) and there are some brilliant restaurants close by. I cannot recommend this place enough!',\n",
       " \"3 friends and I visited 't Materke in April 2014 for 3 nights.  2 of us had stayed before so knew to expect a warm welcome from the owners (Edith and Mario).\\n't Materke is a fantastic B&B situatied in the beautiful and peaceful Belgian countryside just a 10 minute drive from the busier town of Oudenaarde.  In a superb location it is ideal to get out into the countryside by foot or by bike.  \\nEdith and Mario are lovely and couldn't have been more helpful during our stay (from looking up train times to giving us an earlier than normal breakfast).  The rooms are comfortable, clean and have plenty of room for bags and spreading out.\\nBreakfast is incredible and a highlight of any stay at 't Materke.  Ideal to fuel a day's bike riding.  Mario and Edith also have secure storage for bikes at 't Materke so perfect for visiting cyclists.\\nWe will definitely be visiting again!\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of single Sentences found in all available Text\n",
    "sentence_list = sent_tokenize(\". \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I visited this b&b during a short trip to ride the famous Belgian pavé and it was perfect.',\n",
       " 'The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght sentences: 60 \n",
      " Length sentence_list: 435\n"
     ]
    }
   ],
   "source": [
    "print('Lenght sentences:', len(sentences),'\\n', 'Length sentence_list:', len(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input Reduction\n",
    "\n",
    "- Make a List with List with tokenized sentences\n",
    "- check length\n",
    "- go over one item (length) many times and remove item at index\n",
    "- append item to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "i = 0\n",
    "for sentence in sentence_list:\n",
    "    tok_sentences.append(sentence_list[i].split(' '))\n",
    "    i += 1\n",
    "\n",
    "len(tok_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'visited', 'this', 'b&b', 'during', 'a', 'short', 'trip', 'to', 'ride', 'the', 'famous', 'Belgian', 'pavé', 'and', 'it', 'was', 'perfect.'], ['The', 'owners', 'were', 'really', 'lovely', 'people,', 'the', 'room', 'was', 'very', 'comfortable', 'and', 'the', 'breakfast', 'was', 'a', 'delicious', 'feast-', 'ideal', 'for', 'big', 'days', 'out', 'on', 'the', 'bicycle!']]\n"
     ]
    }
   ],
   "source": [
    "print(tok_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tok_sentence):\n",
    "    sentence = ' '.join(tok_sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dropped_sentence_at_pos(sent,token):\n",
    "    tok_mod_sentence = sent.copy()    \n",
    "    tok_mod_sentence.pop(token)\n",
    "    return tok_mod_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "\n",
    "\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = detokenize(tok_sentences[sent])\n",
    "    modified_sentences = []\n",
    "# go over token in sentence\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_sentence': 'All in all, a gem!', 'modified_sentences': [('All', 'in all, a gem!'), ('in', 'All all, a gem!'), ('all,', 'All in a gem!'), ('a', 'All in all, gem!'), ('gem!', 'All in all, a')]}\n"
     ]
    }
   ],
   "source": [
    "print(sentence_packages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juhuuuuu :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict with BERT for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    " sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcccd985ccf4360a6e4f298aa0a1e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b31b9f6d0734f418b13e01398136999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8191e95ef2bf4ad095ad7930f74ebec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45812880992889404"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, 'this was wonderful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43518027663230896"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, 'this was shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visited', 'were', 'close', 'enough!.', 'I', 'Mario).', 'Oudenaarde.', 'bike.', 'breakfast).', 'and', \"'t\", 'Ideal', 'storage', 'again!.', '-', 'already', 'bathroom', 'options.', 'suggest', 'says', 'tiny.', 'to', '(in', 'noisy.', 'time', 'city.', 'went', 'right:', 'fair,', 'a', 'adequate.', 'breakfast', 'bit', 'outside', 'after', 'Breakfast', 'staff.', 'I', 'clean,', 'fabulous', 'fee', 'Dogs', 'Across', 'car,', 'floors.', 'bed.', 'Breakfast', 'doubt', 'gratefully.', 'enjoyed', 'shopping', 'close', 'friendly', 'Refreshing', 'I', 'shopping', 'close', 'friendly', 'Refreshing', 'I', 'nights.', 'old', 'woman', 'staff', 'age', 'Would', 'see..', 'did', 'places', 'road', 'serviceable.', 'dinner.', '100%.', 'it', 'find.', 'rates,', 'option', 'motel', 'rooms!!', 'rooms.', 'Drive', 'Stayed', 'motel', 'town', 'travel', 'hotel', 'location,', 'rooms,', '(with', 'discordant', 'Cleanliness', 'bit', 'nice..', 'balcony,the', 'our', '2', 'We', 'lugging', 'water...\"You', 'town', 'quaint', 'Busset,', 'informing', 'Michelin', '\"No', 'of', 'was', 'I', 'was', 'they', 'makes', 'People', 'chicken..', 'meal.', 'I', 'dish..', 'left', 'was', 'my', 'Black', 'they', 'river.', 'pleasant.', 'always,', 'Toilets', 'fridge', 'elevator,', 'amazing..', 'and', 'was', 'sofa', 'whose', 'make', 'has', 'not', 'all', 'me,', 'eggs', 'road..', \"France's\", 'the', 'Maitre', 'gardens.', 'menu,', 'flights', 'ships', 'rooms', 'were', 'up.', '(in', 'was', 'breads', 'hotel', 'Stayed', 'speaking', 'steps', 'was', 'good,', 'recommended.', 'appreciated.', 'good', 'helpful.', 'comfortable', 'Hotel', 'Rhein.', 'in', 'You', 'did', 'delicious', 'The', 'night..', 'friendly.', 'literally!', 'is', 'Room', 'Rooms', 'I’m', 'internet.', 'Complimentary', '.', 'Chose', 'from', 'It', 'However', 'room', 'fluorescent', 'breakfast,', 'was', 'a', 'again', 'Our', 'in', 'Or,', 'this,', 'lacking', 'might', '.', 'seaside.', 'really', 'included.', \"you're\", 'I', 'do', 'hotel', 'recommend', 'nice', 'is', 'recommend', 'learning', 'pousada', 'Comfortable,', 'were', 'I', 'hotel!!?)', 'But', 'oblivious.', 'tried,', 'accept', 'screamed', 'You', 'It', 'own', 'say', 'did', 'you', 'fab', 'hosts', 'at', 'balcony.', 'comfortable', 'pool!', 'all,', 'a', 'was', 'view', 'pool..', 'and', 'There', 'house,', 'Bed', 'host', 'request.', 'Located', 'very', 'on', 'comfy.', 'fi.', 'owners', 'good', 'difficult', 'show', 'longer.', '.', 'stay', 'and', 'Rita', 'welcoming.', 'clean.', 'morning.', 'Portugueses.', 'day,', 'find,', 'island.', 'you', 'experience.', 'back.', '.', 'staying', 'to', 'talk', 'are', \"wasn't\", \"It's\", 'We', 'recommend!!.', 'along', 'drive', 'live', 'delicious!', 'owners,', 'recommend', '(A', 'needed', 'LOL!', 'restaurants', 'Excellent', 'stay', 'thought', 'This', 'hotel.', 'room', 'Would', 'location.', 'Friendly', 'e.g.', 'bathroom', 'were', 'night.', 'cloth', 'scraped', '4', 'were', 'hotel', 'hotel', 'centre.', 'what', 'Germany.', 'Traveled', 'different', 'is', 'Interrupt,', 'is', 'recommend', 'but', 'clean.', 'value.', 'serves', 'Staff', 'and', 'No', 'The', 'room', 'mention', '.', 'everything', 'town),', 'stay!.', 'staff', 'found', 'exit..', 'surprised', 'about', \"didn't\", 'have', 'Internet', 'a', '(except', 'value', '30', 'here.', 'do.', 'hostels.', 'Quito:', \"wouldn't\", 'may', 'delicious!', 'Quito.', 'enough.', 'Love', 'Love', 'and', 'The', 'is', 'you', 'Rooms', 'okay.', 'Wifi', 'cash,', 'here,', 'breakfast,', 'a', 'stay', 'a', '.', 'stayed', 'that', 'He', 'got', 'from', 'Overall,', 'fine..', 'view.', 'There', 'breakfast', 'is', 'minimal..', 'hotel.', '(tiny', 'best', 'Preto', 'reservations', 'recommendations.', 'surprisingly', 'The', 'The', 'delighted', 'four).', 'rugs?', 'was', 'with', 'sink.', 'of', 'faced', 'bright', 'any', 'smallish', 'television', 'CNN).', 'was', 'found', 'Bathroom', 'super', 'most', '(included).', 'juices.', 'eggs', 'breakfast', 'take', 'also', 'to', 'we', 'etc.', 'night,', 'silent.', 'However,', 'beds', 'award', 'rooms', 'English.', 'how', 'The', 'the', 'reasonable', 'splendid', 'PM).', 'features', 'than', 'more', 'recommend', 'Rooms', 'comfort...this', 'We', 'was', 'now.', 'gem!']\n"
     ]
    }
   ],
   "source": [
    "important_words = []\n",
    "\n",
    "for package in sentence_packages:\n",
    "    original_sentence = package['original_sentence']\n",
    "    # print('new package: ' + original_sentence)\n",
    "\n",
    "    original_result = predict_sentiment(model, tokenizer, original_sentence)\n",
    "    highest_relative = 0\n",
    "    highest_relative_word = None\n",
    "\n",
    "    for item in package['modified_sentences']:\n",
    "        word = item[0]\n",
    "        sentence = item[1]\n",
    "        modified_result = predict_sentiment(model, tokenizer, sentence)\n",
    "        relative = abs(original_result - modified_result)\n",
    "# OBACHT >=\n",
    "        if relative >= highest_relative:\n",
    "            highest_relative = relative\n",
    "            highest_relative_word = word\n",
    "            \n",
    "    important_words.append(highest_relative_word)\n",
    "\n",
    "assert(len(important_words)==len(sentence_packages))    \n",
    "print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'important_words' (list)\n",
      "Stored 'sentence_packages' (list)\n"
     ]
    }
   ],
   "source": [
    "%store important_words\n",
    "%store sentence_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
