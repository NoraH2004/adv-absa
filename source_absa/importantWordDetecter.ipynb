{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset and pick 60 Documents for now\n",
    "2. Do sentence splitting\n",
    "3. Leave One Out (LOO)\n",
    "\n",
    "4. Find important Word through Prediciton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import utils.text_processing as tp\n",
    "import utils.dataloader as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and pick first 60 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SemEval_2015_laptops/absa_15_laptops_train_data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, aspect_category_sentiments, (idx2aspectlabel, idx2sentilabel), cats = dl.semeval_to_aspectsentiment_hr(filename)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Splitting\n",
    "- have list with splitted sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leave One Out (LOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'computer', 'is', 'absolutely', 'AMAZING!!!']\n"
     ]
    }
   ],
   "source": [
    "# List of Lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tok_sentences.append(sentence.split(' '))\n",
    "\n",
    "len(tok_sentences)\n",
    "print(tok_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = tp.detokenize(tok_sentences[sent])\n",
    "    modified_sentences = []\n",
    "# go over token in sentence\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = tp.get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], tp.detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_sentence': 'This computer is absolutely AMAZING!!!', 'modified_sentences': [('This', 'computer is absolutely AMAZING!!!'), ('computer', 'This is absolutely AMAZING!!!'), ('is', 'This computer absolutely AMAZING!!!'), ('absolutely', 'This computer is AMAZING!!!'), ('AMAZING!!!', 'This computer is absolutely')]}, {'original_sentence': '10 plus hours of battery...', 'modified_sentences': [('10', 'plus hours of battery...'), ('plus', '10 hours of battery...'), ('hours', '10 plus of battery...'), ('of', '10 plus hours battery...'), ('battery...', '10 plus hours of')]}, {'original_sentence': 'super fast processor and really nice graphics card..', 'modified_sentences': [('super', 'fast processor and really nice graphics card..'), ('fast', 'super processor and really nice graphics card..'), ('processor', 'super fast and really nice graphics card..'), ('and', 'super fast processor really nice graphics card..'), ('really', 'super fast processor and nice graphics card..'), ('nice', 'super fast processor and really graphics card..'), ('graphics', 'super fast processor and really nice card..'), ('card..', 'super fast processor and really nice graphics')]}, {'original_sentence': 'super fast processor and really nice graphics card..', 'modified_sentences': [('super', 'fast processor and really nice graphics card..'), ('fast', 'super processor and really nice graphics card..'), ('processor', 'super fast and really nice graphics card..'), ('and', 'super fast processor really nice graphics card..'), ('really', 'super fast processor and nice graphics card..'), ('nice', 'super fast processor and really graphics card..'), ('graphics', 'super fast processor and really nice card..'), ('card..', 'super fast processor and really nice graphics')]}, {'original_sentence': 'and plenty of storage with 250 gb(though I will upgrade this and the ram..)', 'modified_sentences': [('and', 'plenty of storage with 250 gb(though I will upgrade this and the ram..)'), ('plenty', 'and of storage with 250 gb(though I will upgrade this and the ram..)'), ('of', 'and plenty storage with 250 gb(though I will upgrade this and the ram..)'), ('storage', 'and plenty of with 250 gb(though I will upgrade this and the ram..)'), ('with', 'and plenty of storage 250 gb(though I will upgrade this and the ram..)'), ('250', 'and plenty of storage with gb(though I will upgrade this and the ram..)'), ('gb(though', 'and plenty of storage with 250 I will upgrade this and the ram..)'), ('I', 'and plenty of storage with 250 gb(though will upgrade this and the ram..)'), ('will', 'and plenty of storage with 250 gb(though I upgrade this and the ram..)'), ('upgrade', 'and plenty of storage with 250 gb(though I will this and the ram..)'), ('this', 'and plenty of storage with 250 gb(though I will upgrade and the ram..)'), ('and', 'and plenty of storage with 250 gb(though I will upgrade this the ram..)'), ('the', 'and plenty of storage with 250 gb(though I will upgrade this and ram..)'), ('ram..)', 'and plenty of storage with 250 gb(though I will upgrade this and the')]}]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_packages[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1974 1974\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_packages),len(aspect_category_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATTERY#OPERATION_PERFORMANCE': 'POS'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_category_sentiments[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict with BERT for ABSA Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nora/code/adv-absa/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Initializing Predictor\n",
      "Loading model models/en-laptops-absa\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "Config loaded from models/en-laptops-absa/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from absa import Predictor\n",
    "from security import Authorization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pred = Predictor(os.path.join('models','en-laptops-absa'))\n",
    "\n",
    "key = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTk1OTU3MDYsInN1YiI6IkFsZXhhbmRlciBSaWV0emxlciIsImlzcyI6ImRlZXBvcGluaW9uLmFpIiwibGFuZ3VhZ2VzIjpbIioiXSwiZnVuY3Rpb25hbGl0eSI6WyIqLyoiXSwiaWF0IjoxNTg4MDU5NzA2fQ.Qz5VPxBIWmmUUpNUp29jw1IKL8TYS_I0vrP_LRWZ9v09tueKHvSddoa8lwjFGi6plAtt6j0w6RiCnSAiw5djQJBXaY40TL36OFjddRrS97zstyizLrXKigQZRqN0w9j53OTV9ViJSXZ8itPLs7bt0KkTsFxoO7gqzC6--SR63c50KS4JQNXCm0an6bePGAtL6OtYABCeLp-TQaR4BfMsqvbBS5T3NSOx65ZPc5COXHZdzRN3gpdc-FXwzRmhzk8LcP4O4tZhxqHUD4u5Rx6sHiCKXULsS_-_hg4344_6taK3UX5IM5h50uXWdLtZ8d-otpZMM0sZijy9XT4jz-mBd_Xzg8nOcHz-8CZXra6NBNgBxpZkJTU_MekZwXKoNE7ktEd5xMruqaut0E_nXXeh32okbuqJ6fmb5F6VQzHBK5Z9Y9WU79tDs5NK9q_zFhLh7ldJKBusCQrB8ADzDs_eBTXaxfMhi0pbFFZWrzIfDce3vrEdyQEXqo8vkrxTzR1YDg7aV47md_L309PolwVM66C6KmnKOT-FVCdIspW96iXoBJ8y7nAkYEM41u5xjqvK39qfmfqA5QeVQXUvBoU9XU0CH1pU6rmnsIpIFphBl598qqIynWWOfdaIk6CRTo-CTzPk06JY8XIuuBayJcbN26MAMKtyeAy7KMfXWmIY3DY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates all packages and evaluates all sentence variations.\n",
    "For each package (=original sentence) we find the important_words that will change the result.\n",
    "Expected output:\n",
    "[['word1', 'word2'],['word3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_packeges = sentence_packages[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original_sentence': 'This computer is absolutely AMAZING!!!',\n",
       "  'modified_sentences': [('This', 'computer is absolutely AMAZING!!!'),\n",
       "   ('computer', 'This is absolutely AMAZING!!!'),\n",
       "   ('is', 'This computer absolutely AMAZING!!!'),\n",
       "   ('absolutely', 'This computer is AMAZING!!!'),\n",
       "   ('AMAZING!!!', 'This computer is absolutely')]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_packeges[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    }
   ],
   "source": [
    "original_result = pred.predict([original_sentence], key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'aspect': 'Laptop (general)',\n",
       "   'sentiment': 'POS',\n",
       "   'text': \"Other than that I don't have one complaint in the world!\",\n",
       "   'span': [0, 56]}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICE=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_sentence': 'This computer is absolutely AMAZING!!!', 'modified_sentences': [('This', 'computer is absolutely AMAZING!!!'), ('computer', 'This is absolutely AMAZING!!!'), ('is', 'This computer absolutely AMAZING!!!'), ('absolutely', 'This computer is AMAZING!!!'), ('AMAZING!!!', 'This computer is absolutely')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    }
   ],
   "source": [
    "important_words = []\n",
    "\n",
    "for package in test_packeges:\n",
    "    print(package)\n",
    "    original_sentence = package['original_sentence']   \n",
    "        \n",
    "    original_result = pred.predict([original_sentence], key)[0][0]\n",
    "    original_aspect = original_result['aspect']\n",
    "    original_sentiment = original_result['sentiment']\n",
    "    \n",
    "    modified_sentences = []\n",
    "    \n",
    "    for item in package['modified_sentences']:\n",
    "        modified_sentences.append(item[1])    \n",
    "        \n",
    "        \n",
    "        important_words = []\n",
    "    for modified_sentence in modified_sentences:\n",
    "        \n",
    "        modified_result = pred.predict([modified_sentence], key)[0][0]\n",
    "        modified_aspect = modified_result['aspect']\n",
    "        modified_sentiment = modified_result['sentiment']\n",
    "        modified_word = item[0]\n",
    "        \n",
    "        if modified_aspect != original_aspect or modified_sentiment != original_sentiment:\n",
    "            important_words.append(modified_word)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #important_words_sentence = []\n",
    "    #for sentence in modified_sentences:\n",
    "            # TODO READ FROM MODEL\n",
    "     #   modified_result = pred.predict([sentence[1]], key);\n",
    "                \n",
    "     #   modified_aspect = modified_result[0][0]['aspect']\n",
    "      \n",
    "        #modified_sentiment = modified_result[0][0]['sentiment']\n",
    "        \n",
    "        # if original_aspect !=  modified_aspect or original_sentiment != modified_sentiment:\n",
    "          #    important_words_sentence.append(sentence[0])\n",
    "        \n",
    "  #  important_words.append(important_words_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9840f452ed2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodified_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'item' is not defined"
     ]
    }
   ],
   "source": [
    "modified_word = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aspect': 'Laptop (general)',\n",
       " 'sentiment': 'POS',\n",
       " 'text': 'This computer is absolutely',\n",
       " 'span': [0, 27]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'important_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-700801813e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportant_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'important_words' is not defined"
     ]
    }
   ],
   "source": [
    "important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store important_words\n",
    "%store sentence_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
