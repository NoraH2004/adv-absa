{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset and pick 60 Documents for now\n",
    "2. Do sentence splitting\n",
    "3. Leave One Out (LOO)\n",
    "\n",
    "4. Find important Word through Prediciton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import utils.text_processing as tp\n",
    "import utils.dataloader as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and pick first 60 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absa_15_laptops_train_data.xml\n"
     ]
    }
   ],
   "source": [
    "!ls SemEval_2015_laptops/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SemEval_2015_laptops/absa_15_laptops_train_data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1974"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, aspect_category_sentiments, (idx2aspectlabel, idx2sentilabel), cats = dl.semeval_to_aspectsentiment_hr(filename)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Splitting\n",
    "- have list with splitted sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leave One Out (LOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'computer', 'is', 'absolutely', 'AMAZING!!!']\n"
     ]
    }
   ],
   "source": [
    "# List of Lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tok_sentences.append(sentence.split(' '))\n",
    "\n",
    "len(tok_sentences)\n",
    "print(tok_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = tp.detokenize(tok_sentences[sent])\n",
    "    modified_sentences = []\n",
    "# go over token in sentence\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = tp.get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], tp.detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'original_sentence': 'This computer is absolutely AMAZING!!!', 'modified_sentences': [('This', 'computer is absolutely AMAZING!!!'), ('computer', 'This is absolutely AMAZING!!!'), ('is', 'This computer absolutely AMAZING!!!'), ('absolutely', 'This computer is AMAZING!!!'), ('AMAZING!!!', 'This computer is absolutely')]}, {'original_sentence': '10 plus hours of battery...', 'modified_sentences': [('10', 'plus hours of battery...'), ('plus', '10 hours of battery...'), ('hours', '10 plus of battery...'), ('of', '10 plus hours battery...'), ('battery...', '10 plus hours of')]}, {'original_sentence': 'super fast processor and really nice graphics card..', 'modified_sentences': [('super', 'fast processor and really nice graphics card..'), ('fast', 'super processor and really nice graphics card..'), ('processor', 'super fast and really nice graphics card..'), ('and', 'super fast processor really nice graphics card..'), ('really', 'super fast processor and nice graphics card..'), ('nice', 'super fast processor and really graphics card..'), ('graphics', 'super fast processor and really nice card..'), ('card..', 'super fast processor and really nice graphics')]}, {'original_sentence': 'super fast processor and really nice graphics card..', 'modified_sentences': [('super', 'fast processor and really nice graphics card..'), ('fast', 'super processor and really nice graphics card..'), ('processor', 'super fast and really nice graphics card..'), ('and', 'super fast processor really nice graphics card..'), ('really', 'super fast processor and nice graphics card..'), ('nice', 'super fast processor and really graphics card..'), ('graphics', 'super fast processor and really nice card..'), ('card..', 'super fast processor and really nice graphics')]}, {'original_sentence': 'and plenty of storage with 250 gb(though I will upgrade this and the ram..)', 'modified_sentences': [('and', 'plenty of storage with 250 gb(though I will upgrade this and the ram..)'), ('plenty', 'and of storage with 250 gb(though I will upgrade this and the ram..)'), ('of', 'and plenty storage with 250 gb(though I will upgrade this and the ram..)'), ('storage', 'and plenty of with 250 gb(though I will upgrade this and the ram..)'), ('with', 'and plenty of storage 250 gb(though I will upgrade this and the ram..)'), ('250', 'and plenty of storage with gb(though I will upgrade this and the ram..)'), ('gb(though', 'and plenty of storage with 250 I will upgrade this and the ram..)'), ('I', 'and plenty of storage with 250 gb(though will upgrade this and the ram..)'), ('will', 'and plenty of storage with 250 gb(though I upgrade this and the ram..)'), ('upgrade', 'and plenty of storage with 250 gb(though I will this and the ram..)'), ('this', 'and plenty of storage with 250 gb(though I will upgrade and the ram..)'), ('and', 'and plenty of storage with 250 gb(though I will upgrade this the ram..)'), ('the', 'and plenty of storage with 250 gb(though I will upgrade this and ram..)'), ('ram..)', 'and plenty of storage with 250 gb(though I will upgrade this and the')]}]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_packages[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1974 1974\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_packages),len(aspect_category_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BATTERY#OPERATION_PERFORMANCE': 'POS'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_category_sentiments[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict with BERT for ABSA Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Predictor\n",
      "Loading model models/en-laptops-absa\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "Config loaded from models/en-laptops-absa/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from absa import Predictor\n",
    "from security import Authorization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pred = Predictor(os.path.join('models','en-laptops-absa'))\n",
    "\n",
    "key = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTk1OTU3MDYsInN1YiI6IkFsZXhhbmRlciBSaWV0emxlciIsImlzcyI6ImRlZXBvcGluaW9uLmFpIiwibGFuZ3VhZ2VzIjpbIioiXSwiZnVuY3Rpb25hbGl0eSI6WyIqLyoiXSwiaWF0IjoxNTg4MDU5NzA2fQ.Qz5VPxBIWmmUUpNUp29jw1IKL8TYS_I0vrP_LRWZ9v09tueKHvSddoa8lwjFGi6plAtt6j0w6RiCnSAiw5djQJBXaY40TL36OFjddRrS97zstyizLrXKigQZRqN0w9j53OTV9ViJSXZ8itPLs7bt0KkTsFxoO7gqzC6--SR63c50KS4JQNXCm0an6bePGAtL6OtYABCeLp-TQaR4BfMsqvbBS5T3NSOx65ZPc5COXHZdzRN3gpdc-FXwzRmhzk8LcP4O4tZhxqHUD4u5Rx6sHiCKXULsS_-_hg4344_6taK3UX5IM5h50uXWdLtZ8d-otpZMM0sZijy9XT4jz-mBd_Xzg8nOcHz-8CZXra6NBNgBxpZkJTU_MekZwXKoNE7ktEd5xMruqaut0E_nXXeh32okbuqJ6fmb5F6VQzHBK5Z9Y9WU79tDs5NK9q_zFhLh7ldJKBusCQrB8ADzDs_eBTXaxfMhi0pbFFZWrzIfDce3vrEdyQEXqo8vkrxTzR1YDg7aV47md_L309PolwVM66C6KmnKOT-FVCdIspW96iXoBJ8y7nAkYEM41u5xjqvK39qfmfqA5QeVQXUvBoU9XU0CH1pU6rmnsIpIFphBl598qqIynWWOfdaIk6CRTo-CTzPk06JY8XIuuBayJcbN26MAMKtyeAy7KMfXWmIY3DY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates all packages and evaluates all sentence variations.\n",
    "For each package (=original sentence) we find the important_words that will change the result.\n",
    "Expected output:\n",
    "[['word1', 'word2'],['word3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [{'aspect': 'Temperature', 'sentiment': 'POS', 'text': 'it is very cold', 'span': [0, 15]}], [{'aspect': 'Screen', 'sentiment': 'NEG', 'text': 'the monitor has funny colors', 'span': [0, 28]}]]\n"
     ]
    }
   ],
   "source": [
    "documents = ['hello', 'it is very cold', 'the monitor has funny colors']\n",
    "print(pred.predict(documents, key));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_packeges = [{'original_sentence': 'This computer is absolutely AMAZING!!!', 'modified_sentences': [('This', 'computer is absolutely AMAZING!!!'), ('computer', 'This is absolutely AMAZING!!!'), ('is', 'This computer absolutely AMAZING!!!'), ('absolutely', 'This computer is AMAZING!!!'), ('AMAZING!!!', 'This computer is absolutely')]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    }
   ],
   "source": [
    "important_words = []\n",
    "\n",
    "\n",
    "for package in test_packeges:\n",
    "    original_sentence = package['original_sentence']\n",
    "    modified_sentences = package['modified_sentences']\n",
    "    \n",
    "    original_result = pred.predict([original_sentence], key);\n",
    "    \n",
    "    important_words_sentence = []\n",
    "    for sentence in modified_sentences:\n",
    "            # TODO READ FROM MODEL\n",
    "        input = sentence[1]\n",
    "        modified_result = pred.predict([sentence[1]], key);\n",
    "        print(modified_result)\n",
    "        \n",
    "        # if original_result['aspect'] !=  modified_result['aspect'] or original_result['sentiment'] != modified_result['sentiment']:\n",
    "        #       important_words_sentence.append(sentence[0])\n",
    "        \n",
    "    important_words.append(important_words_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'aspect': 'Laptop (general)',\n",
       "   'sentiment': 'POS',\n",
       "   'text': 'This computer is absolutely AMAZING!!!',\n",
       "   'span': [0, 38]}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'aspect': 'Laptop (general)',\n",
       "   'sentiment': 'POS',\n",
       "   'text': 'This computer is absolutely',\n",
       "   'span': [0, 27]}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This computer is absolutely AMAZING!!!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentence = 'This computer is absolutely AMAZING!!!'\n",
    "original_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.predict(original_sentence, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_results = [{'OOO1#OOO2': 'OOOOO'},\n",
    " {'BATTERY#OOOOO': 'OOOOO'},\n",
    " {'OOOOO#OPERATION_PERFORMANCE': 'POS', 'GRAPHICS#GENERAL': 'POS'},\n",
    " {'CPU#OPERATION_PERFORMANCE': 'POS', 'GRAPHICS#GENERAL': 'POS'},\n",
    " {'HARD_DISC#DESIGN_FEATURES': 'OOOOO'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_results = [{'LAPTOP#GENERAL': 'POS'},\n",
    " {'BATTERY#OPERATION_PERFORMANCE': 'POS'},\n",
    " {'CPU#OPERATION_PERFORMANCE': 'POS', 'GRAPHICS#GENERAL': 'POS'},\n",
    " {'CPU#OPERATION_PERFORMANCE': 'POS', 'GRAPHICS#GENERAL': 'POS'},\n",
    " {'HARD_DISC#DESIGN_FEATURES': 'POS'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_item in modified_results:\n",
    "    for key, value in mod_item.items():\n",
    "        mod_entity = key.split(\"#\")[0]\n",
    "        mod_attribute = key.split(\"#\")[1]\n",
    "        mod_sentiment = value\n",
    "\n",
    "for orig_item in original_results:\n",
    "    for key, value in orig_item.items():\n",
    "        orig_entity = key.split(\"#\")[0]\n",
    "        orig_attribute = key.split(\"#\")[1]\n",
    "        orig_sentiment = value\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = []\n",
    "\n",
    "for e, package in enumerate(sentence_packages):    \n",
    "    original_sentence = package['original_sentence']\n",
    "    # print('new package: ' + original_sentence)\n",
    "\n",
    "    original_result = aspect_category_sentiments[e]\n",
    "    for key, value in original_result.items():\n",
    "        original_entity = key.split(\"#\")[0]\n",
    "        original_attribute = key.split(\"#\")[1]\n",
    "        original_sentiment = value\n",
    "\n",
    "    for item in package['modified_sentences']:\n",
    "        word = item[0]\n",
    "        sentence = item[1]\n",
    "        # modified_result = tp.predict_sentiment(model, tokenizer, sentence)\n",
    "        for key, value in modified_result.items():\n",
    "            mod_entity = key.split(\"#\")[0]\n",
    "            mod_attribute = key.split(\"#\")[1]\n",
    "            mod_sentiment = value\n",
    "        \n",
    "        modification = 0\n",
    "        if mod_entity != original_entity:\n",
    "            modification += 1\n",
    "        if mod_attribute != original_attribute:\n",
    "            modification += 1\n",
    "        if mod_sentiment != original_sentiment:\n",
    "            modification += 1\n",
    "        \n",
    "        if modification > 2:\n",
    "            important_words_nr1.append(word)\n",
    "        elif modification == 2:\n",
    "            important_words_nr2.append(word)\n",
    "        else: \n",
    "            important_words_nr3.append(word)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    important_words.append(highest_relative_word)\n",
    "\n",
    "assert(len(important_words)==len(sentence_packages))    \n",
    "# print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store important_words\n",
    "%store sentence_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
