{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset and pick 60 Documents for now\n",
    "2. Do sentence splitting\n",
    "3. Leave One Out (LOO)\n",
    "\n",
    "4. Find important Word through Prediciton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "import utils.text_processing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and pick first 60 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tp.load_jsonline(os.path.join('data', 'items_reviews_18.jl'), 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_id': 3321611,\n",
       " 'source_id': 277605655,\n",
       " 'title': 'This place is incredible!',\n",
       " 'text': 'I visited this b&b during a short trip to ride the famous Belgian pavÃ© and it was perfect. The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle! It is in a really good location for riding or driving into Oudenaarde (approx 10 mins) and there are some brilliant restaurants close by. I cannot recommend this place enough!',\n",
       " 'user_rating': 5,\n",
       " 'lang': 'en',\n",
       " '_type': 'TripAdvisorHotelReviewItem'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Splitting\n",
    "- have list with text items\n",
    "- have list with splitted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Text form Reviews\n",
    "sentences = []\n",
    "for obj in data:\n",
    "    sentences.append(obj[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of single Sentences found in all available Text\n",
    "sentence_list = sent_tokenize(\". \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght sentences: 60 \n",
      " Length sentence_list: 435\n"
     ]
    }
   ],
   "source": [
    "print('Lenght sentences:', len(sentences), '\\n', 'Length sentence_list:', len(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leave One Out (LOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "i = 0\n",
    "for sentence in sentence_list:\n",
    "    tok_sentences.append(sentence_list[i].split(' '))\n",
    "    i += 1\n",
    "\n",
    "len(tok_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = tp.detokenize(tok_sentences[sent])\n",
    "    modified_sentences = []\n",
    "# go over token in sentence\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = tp.get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], tp.detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juhuuuuu :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict with BERT for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105879"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model.eval();\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = []\n",
    "\n",
    "for package in sentence_packages:    \n",
    "    original_sentence = package['original_sentence']\n",
    "    # print('new package: ' + original_sentence)\n",
    "\n",
    "    original_result = tp.predict_sentiment(model, tokenizer, original_sentence)\n",
    "    highest_relative = 0\n",
    "    highest_relative_word = None\n",
    "\n",
    "    for item in package['modified_sentences']:\n",
    "        word = item[0]\n",
    "        sentence = item[1]\n",
    "        modified_result = tp.predict_sentiment(model, tokenizer, sentence)\n",
    "        relative = abs(original_result - modified_result)\n",
    "# OBACHT >=\n",
    "        if relative >= highest_relative:\n",
    "            highest_relative = relative\n",
    "            highest_relative_word = word\n",
    "            \n",
    "    important_words.append(highest_relative_word)\n",
    "\n",
    "assert(len(important_words)==len(sentence_packages))    \n",
    "# print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'important_words' (list)\n",
      "Stored 'sentence_packages' (list)\n"
     ]
    }
   ],
   "source": [
    "%store important_words\n",
    "%store sentence_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
