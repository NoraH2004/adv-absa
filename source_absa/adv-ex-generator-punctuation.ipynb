{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for crafting Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import utils.text_processing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from important word detector\n",
    "%store -r important_words_packages\n",
    "%store -r sentence_packages\n",
    "%store -r loo_results\n",
    "\n",
    "%store -r important_words_packages_dev\n",
    "%store -r sentence_packages_dev\n",
    "%store -r loo_results_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ;Method, 3: ?punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create modified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n"
     ]
    }
   ],
   "source": [
    "# Original Sentences\n",
    "\n",
    "original_sentences = []\n",
    "for package in sentence_packages:\n",
    "    original_sentences.append(package['original_sentence'])\n",
    "\n",
    "print(len(important_words_packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total modified sentences:  2555\n"
     ]
    }
   ],
   "source": [
    "# Important Words --> Modified Words\n",
    "\n",
    "modified_words_packages = []\n",
    "length = 0\n",
    "\n",
    "for important_words in important_words_packages:\n",
    "    modified_words = []\n",
    "    #print(important_words)\n",
    "    \n",
    "    for word in important_words:\n",
    "        modified_word_variances = []\n",
    "        modified_word_variances.append(tp.to_punctuation(word))\n",
    "        #print(modified_word_variances)\n",
    "        modified_words.append(modified_word_variances)\n",
    "        length += 1\n",
    "\n",
    "    modified_words_packages.append(modified_words)\n",
    "        \n",
    "#print(modified_words_packages[0])\n",
    "print('Total modified sentences: ', length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences, modified_sentence_packages, len_mod_sent = tp.generate_modified_sentence_packages(original_sentences, important_words_packages, modified_words_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of original sentences:  939\n",
      "# of modified sentence packages:  939\n"
     ]
    }
   ],
   "source": [
    "#print(modified_sentence_packages)\n",
    "print('# of original sentences: ', len(original_sentences))\n",
    "print('# of modified sentence packages: ',len(modified_sentence_packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2555\n"
     ]
    }
   ],
   "source": [
    "lenght = 0\n",
    "for item in modified_sentence_packages:\n",
    "    #lenght += 1\n",
    "    for sentence in item:\n",
    "        lenght += 1\n",
    "print(lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Predictor\n",
      "DEBUG:absa.absa:Initializing Predictor\n",
      "Loading model models/en-laptops-absa\n",
      "DEBUG:absa.utils:Loading model models/en-laptops-absa\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "DEBUG:absa.utils:Config loaded from models/en-laptops-absa/config.json\n",
      "Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "DEBUG:absa.utils:Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "DEBUG:absa.utils:Config loaded from models/en-laptops-absa/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from absa import Predictor\n",
    "from security import Authorization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pred = Predictor(os.path.join('models','en-laptops-absa'))\n",
    "\n",
    "key = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTk1OTU3MDYsInN1YiI6IkFsZXhhbmRlciBSaWV0emxlciIsImlzcyI6ImRlZXBvcGluaW9uLmFpIiwibGFuZ3VhZ2VzIjpbIioiXSwiZnVuY3Rpb25hbGl0eSI6WyIqLyoiXSwiaWF0IjoxNTg4MDU5NzA2fQ.Qz5VPxBIWmmUUpNUp29jw1IKL8TYS_I0vrP_LRWZ9v09tueKHvSddoa8lwjFGi6plAtt6j0w6RiCnSAiw5djQJBXaY40TL36OFjddRrS97zstyizLrXKigQZRqN0w9j53OTV9ViJSXZ8itPLs7bt0KkTsFxoO7gqzC6--SR63c50KS4JQNXCm0an6bePGAtL6OtYABCeLp-TQaR4BfMsqvbBS5T3NSOx65ZPc5COXHZdzRN3gpdc-FXwzRmhzk8LcP4O4tZhxqHUD4u5Rx6sHiCKXULsS_-_hg4344_6taK3UX5IM5h50uXWdLtZ8d-otpZMM0sZijy9XT4jz-mBd_Xzg8nOcHz-8CZXra6NBNgBxpZkJTU_MekZwXKoNE7ktEd5xMruqaut0E_nXXeh32okbuqJ6fmb5F6VQzHBK5Z9Y9WU79tDs5NK9q_zFhLh7ldJKBusCQrB8ADzDs_eBTXaxfMhi0pbFFZWrzIfDce3vrEdyQEXqo8vkrxTzR1YDg7aV47md_L309PolwVM66C6KmnKOT-FVCdIspW96iXoBJ8y7nAkYEM41u5xjqvK39qfmfqA5QeVQXUvBoU9XU0CH1pU6rmnsIpIFphBl598qqIynWWOfdaIk6CRTo-CTzPk06JY8XIuuBayJcbN26MAMKtyeAy7KMfXWmIY3DY\"\n",
    "!export CUDA_VISIBLE_DEVICE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prediction\n",
    "\n",
    "###### Original Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for sentence in original_sentences:\n",
    "    document = {'text': sentence, 'segments':[{'span':[0,0],'text': sentence}]}\n",
    "    documents.append(document)\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "original_predictions = []\n",
    "for result in results:\n",
    "    original_predictions.append(result[0])\n",
    "#print('original_predictions: ', original_predictions)\n",
    "print(len(original_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_results_flattened:  2555\n",
      "2555\n",
      "939\n"
     ]
    }
   ],
   "source": [
    "# 1. create indices for prediction,\n",
    "# 2. flatten modified_sentence_packages and \n",
    "# 3. predict flattened list\n",
    "\n",
    "results = []\n",
    "documents = []\n",
    "package_indices = []\n",
    "package_index = 0\n",
    "for sentence in modified_sentence_packages:\n",
    "    package_index += 1\n",
    "    for word in sentence:\n",
    "        for variant in word:  \n",
    "            package_indices.append(package_index)\n",
    "            document = {'text': variant, 'segments':[{'span':[0,0],'text': variant}]}\n",
    "            documents.append(document)\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "modified_results_flattened = []\n",
    "for result in results:\n",
    "    modified_results_flattened.append(result[0])\n",
    "print('modified_results_flattened: ', len(modified_results_flattened))\n",
    "\n",
    "print(len(package_indices))\n",
    "print(package_index)\n",
    "#print(package_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "# 4. map to lvl2 sentence packages\n",
    "# IMPORTANT: here I loose the last variants level because I do not actually need it\n",
    "# todo: loose levels --> only have one level per sentence when generating modified sentence packages\n",
    "\n",
    "modified_predictions = []\n",
    "modified_sentence = []\n",
    "check = 1\n",
    "for e, result in enumerate(modified_results_flattened):\n",
    "    i = package_indices[e]    \n",
    "    if i == check:\n",
    "        modified_sentence.append(result)\n",
    "    else:\n",
    "        modified_predictions.append(modified_sentence)\n",
    "        modified_sentence = []\n",
    "        modified_sentence.append(result)\n",
    "    check = i\n",
    "modified_predictions.append(modified_sentence)\n",
    "print(len(modified_predictions))\n",
    "#print(modified_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of results to check effectiveness of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "[{'original_sentence': 'When I got the computer back and realizwed it still was not correct HP told me it was out of warranty and now it was my problem.', 'original_result': [{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}], 'modified_sentences': [['When I got the computer, back and realizwed it still was not correct HP told me it was out of warranty and now it was my problem.', 'When I got the computer back and realizwed it still was not correct HP told me it was out of warranty and now it was my problem.,']], 'modified_results': [[[], []]]}]\n"
     ]
    }
   ],
   "source": [
    "# loo_results is a list that displays the modification\n",
    "punctuation_results = []\n",
    "\n",
    "for e, modified_predictions_set in enumerate(modified_predictions):\n",
    "    successfull_modifications_set_txt = []\n",
    "    successfull_modifications_set_aspsent = []\n",
    "    original_result = original_predictions[e]\n",
    "    o_aspect_sentiment = original_result['aspect_sentiments']\n",
    "    \n",
    "    successfull_modifications_txt = []\n",
    "    successfull_modifications_aspsent = []\n",
    "    for modified_result in modified_predictions_set:\n",
    "        m_aspect_sentiment = modified_result['aspect_sentiments']\n",
    "        #print('original_result :', original_result, 'modified_result: ', modified_result)\n",
    "        \n",
    "        if o_aspect_sentiment != m_aspect_sentiment:\n",
    "            #print('original_result :', original_result, 'modified_result: ', modified_result)\n",
    "            successfull_modifications_txt.append(modified_result['text'])\n",
    "            successfull_modifications_aspsent.append(modified_result['aspect_sentiments'])\n",
    "    successfull_modifications_set_txt.append(successfull_modifications_txt)\n",
    "    successfull_modifications_set_aspsent.append(successfull_modifications_aspsent)\n",
    "    \n",
    "    if successfull_modifications_txt:\n",
    "        punctuation_results.append(\n",
    "            {\n",
    "                'original_sentence': original_result['text'],\n",
    "                'original_result': original_result['aspect_sentiments'],\n",
    "                'modified_sentences': successfull_modifications_set_txt,\n",
    "                'modified_results': successfull_modifications_set_aspsent\n",
    "            })\n",
    "        \n",
    "#print(c)   \n",
    "print(len(punctuation_results))\n",
    "print(punctuation_results[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Creation of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_texts = []\n",
    "original_results = []\n",
    "modified_texts = []\n",
    "modified_results = []\n",
    "\n",
    "for item in punctuation_results:\n",
    "    original_texts.append(item['original_sentence'])\n",
    "    original_results.append(item['original_result'])\n",
    "    \n",
    "    modified_text_p_sent = []\n",
    "    modified_result_p_sent = []\n",
    "    for sentence in item['modified_sentences']:\n",
    "        modified_text_p_sent.append(sentence)\n",
    "    for result in item['modified_results']:\n",
    "        modified_result_p_sent.append(result)\n",
    "    modified_texts.append(modified_text_p_sent)\n",
    "    modified_results.append(modified_result_p_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(list(zip(original_texts, original_results, modified_texts, modified_results)),\n",
    "                 columns = ['original_sentence', 'original_prediction', 'modified_sentence', 'modified_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 253 entries, 0 to 252\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   original_sentence    253 non-null    object\n",
      " 1   original_prediction  253 non-null    object\n",
      " 2   modified_sentence    253 non-null    object\n",
      " 3   modified_prediction  253 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 8.0+ KB\n"
     ]
    }
   ],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Creation of adversarial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "advds = tp.filter_unchanged_predictions(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "advds.to_json(r'data/adversarial_dataset_punctuation.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmethod = 'punctuation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Interpretation\n",
    "    \n",
    "1. Sentences in Dataset: Unique sentences in Dataset, where (iwords p. sent >= 1)\n",
    "2. Modifyable Sentences in Dataset: sentences, where iword could be replaced by (mod_words >= 1)\n",
    "3. Modified Sentences to predict: sent in ds * iwords * mod_words --> 100%\n",
    "4. Successfull Modifications: sentences, where the prediction was different than to the original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perturbation Method</th>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tokenizer</th>\n",
       "      <td>nlptown/bert-base-multilingual-uncased-sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>nlptown/bert-base-multilingual-uncased-sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>TripAdvisor Hotel Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output lables</th>\n",
       "      <td>Range from 0 to 4 - 0 = NEG; 4 = POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Items in original dataset</th>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of modifyable items original</th>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Items in adversarial dataset</th>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    0\n",
       "Perturbation Method                                                       punctuation\n",
       "Tokenizer                            nlptown/bert-base-multilingual-uncased-sentiment\n",
       "Model                                nlptown/bert-base-multilingual-uncased-sentiment\n",
       "Dataset                                                     TripAdvisor Hotel Reviews\n",
       "Output lables                                    Range from 0 to 4 - 0 = NEG; 4 = POS\n",
       "Items in original dataset                                                         394\n",
       "Number of modifyable items original                                               394\n",
       "Items in adversarial dataset                                                      394\n",
       "Percentage                                                                        100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_punctuation = tp.generate_results_df(pmethod, ds, advds)\n",
    "results_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-66f2933596e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodified_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for label size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# font size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sdb1/nora/git/adv-absa/venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sdb1/nora/git/adv-absa/venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sdb1/nora/git/adv-absa/venv/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    258\u001b[0m         if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)\n\u001b[1;32m    259\u001b[0m                 and not isinstance(y[0], str)):\n\u001b[0;32m--> 260\u001b[0;31m             raise ValueError('You appear to be using a legacy multi-label data'\n\u001b[0m\u001b[1;32m    261\u001b[0m                              \u001b[0;34m' representation. Sequence of sequences are no'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                              \u001b[0;34m' longer supported; use a binary array or sparse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "array = confusion_matrix(original_predictions, modified_predictions)\n",
    "df_cm = pd.DataFrame(array, range(5), range(5))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt=\"d\", linewidths=.1) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'results_punctuation' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store results_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "successfull_modifications = 0\n",
    "for item in punctuation_results:\n",
    "    for result in item['modified_sentences']:\n",
    "        successfull_modifications += 1\n",
    "        \n",
    "print(successfull_modifications)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_raw_ds = len(sentence_packages)\n",
    "number_of_modifyable_senteces = len(original_sentences)\n",
    "number_of_modified_sentences = len(documents)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pmethod = 'typo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "939\n",
      "2555\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "print(size_raw_ds)\n",
    "print(number_of_modifyable_senteces)\n",
    "print(number_of_modified_sentences)\n",
    "print(successfull_modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
