{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for crafting Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import utils.text_processing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from important word detector\n",
    "%store -r important_words_packages\n",
    "%store -r sentence_packages\n",
    "%store -r loo_results\n",
    "\n",
    "%store -r important_words_packages_dev\n",
    "%store -r sentence_packages_dev\n",
    "%store -r loo_results_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words_packages = important_words_packages_dev\n",
    "sentence_packages = sentence_packages_dev\n",
    "loo_results = loo_results_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methed 2: Mispeeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create Typo-Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','common_typos_wikipedia.txt'), 'r') as f:\n",
    "    typods = f.readlines()\n",
    "\n",
    "typodict = {}\n",
    "      \n",
    "for typod in typods:  \n",
    "    typo = typod.split(\"->\")[0].strip('\\n')\n",
    "    word = typod.split(\"->\")[1].strip('\\n')\n",
    " \n",
    "    for item in word.split(\", \"):\n",
    "        if item in typodict:\n",
    "            tmp_typo = typodict[item]\n",
    "            tmp_typo.append(typo)             \n",
    "            tmp_typo = list(set(tmp_typo)) #filter duplicates\n",
    "            typodict[item] = tmp_typo\n",
    "        else:\n",
    "            typodict.update({item : [typo]})\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original sentences from sentence_packages\n",
    "original_sentences = []\n",
    "for package in sentence_packages:\n",
    "    original_sentences.append(package['original_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate modified words: take Important Words and conduct modification\n",
    "\n",
    "modified_words_packages = []\n",
    "\n",
    "for important_words in important_words_packages:\n",
    "    modified_words = []\n",
    "    \n",
    "    for word in important_words:\n",
    "        modified_word_variances = []\n",
    "        modified_word_variances.append(tp.to_typo(typodict, word))\n",
    "        modified_words.append(modified_word_variances)\n",
    "\n",
    "    modified_words_packages.append(modified_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate modified sentences\n",
    "modifyable_original_sentences, modified_sentence_packages, number_of_modified_sentences = tp.generate_modified_sentence_packages(original_sentences, important_words_packages, modified_words_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of original sentences: 10\n",
      "Total number of modifyable original sentences:  2\n",
      "Total number of modified sentences:  3\n"
     ]
    }
   ],
   "source": [
    "print('Total number of original sentences:', len(original_sentences))\n",
    "print('Total number of modifyable original sentences: ', len(modifyable_original_sentences))\n",
    "print('Total number of modified sentences: ', number_of_modified_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Predictor\n",
      "Loading model models/en-laptops-absa\n",
      "Config loaded from models/en-laptops-absa/config.json\n",
      "Aspects loaded from models/en-laptops-absa/aspects.jsonl\n",
      "Config loaded from models/en-laptops-absa/config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from absa import Predictor\n",
    "from security import Authorization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pred = Predictor(os.path.join('models','en-laptops-absa'))\n",
    "\n",
    "key = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTk1OTU3MDYsInN1YiI6IkFsZXhhbmRlciBSaWV0emxlciIsImlzcyI6ImRlZXBvcGluaW9uLmFpIiwibGFuZ3VhZ2VzIjpbIioiXSwiZnVuY3Rpb25hbGl0eSI6WyIqLyoiXSwiaWF0IjoxNTg4MDU5NzA2fQ.Qz5VPxBIWmmUUpNUp29jw1IKL8TYS_I0vrP_LRWZ9v09tueKHvSddoa8lwjFGi6plAtt6j0w6RiCnSAiw5djQJBXaY40TL36OFjddRrS97zstyizLrXKigQZRqN0w9j53OTV9ViJSXZ8itPLs7bt0KkTsFxoO7gqzC6--SR63c50KS4JQNXCm0an6bePGAtL6OtYABCeLp-TQaR4BfMsqvbBS5T3NSOx65ZPc5COXHZdzRN3gpdc-FXwzRmhzk8LcP4O4tZhxqHUD4u5Rx6sHiCKXULsS_-_hg4344_6taK3UX5IM5h50uXWdLtZ8d-otpZMM0sZijy9XT4jz-mBd_Xzg8nOcHz-8CZXra6NBNgBxpZkJTU_MekZwXKoNE7ktEd5xMruqaut0E_nXXeh32okbuqJ6fmb5F6VQzHBK5Z9Y9WU79tDs5NK9q_zFhLh7ldJKBusCQrB8ADzDs_eBTXaxfMhi0pbFFZWrzIfDce3vrEdyQEXqo8vkrxTzR1YDg7aV47md_L309PolwVM66C6KmnKOT-FVCdIspW96iXoBJ8y7nAkYEM41u5xjqvK39qfmfqA5QeVQXUvBoU9XU0CH1pU6rmnsIpIFphBl598qqIynWWOfdaIk6CRTo-CTzPk06JY8XIuuBayJcbN26MAMKtyeAy7KMfXWmIY3DY\"\n",
    "!export CUDA_VISIBLE_DEVICE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prediction\n",
    "\n",
    "###### Original Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'super fast processor and really nice graphics card..', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'CPU', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}]}, {'text': 'Oh my goodness-I am not a happy camper.', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}]}]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for sentence in modifyable_original_sentences:\n",
    "    document = {'text': sentence, 'segments':[{'span':[0,0],'text': sentence}]}\n",
    "    documents.append(document)\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "original_predictions = []\n",
    "for result in results:\n",
    "    original_predictions.append(result[0])\n",
    "print(original_predictions)\n",
    "print(len(original_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n",
      "DEBUG:security.authorization:Running authorization for token for functionality Analysis/Aspect-Sentiments and language None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'super fast processer and really nice graphics card..', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'CPU', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}]}, {'text': 'Oh my goodness-I am onot a happy camper.', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'Oh my goodness-I am nto a happy camper.', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 1. create indices for prediction,\n",
    "# 2. flatten modified_sentence_packages and \n",
    "# 3. predict flattened list\n",
    "\n",
    "results = []\n",
    "documents = []\n",
    "package_indices = []\n",
    "package_index = 0\n",
    "for sentence in modified_sentence_packages:\n",
    "    package_index += 1\n",
    "    for word in sentence:\n",
    "        for variant in word:  \n",
    "            package_indices.append(package_index)\n",
    "            document = {'text': variant, 'segments':[{'span':[0,0],'text': variant}]}\n",
    "            documents.append(document)\n",
    "    \n",
    "results = pred.predict(documents, key, with_segments=True)\n",
    "\n",
    "modified_results_flattened = []\n",
    "for result in results:\n",
    "    modified_results_flattened.append(result[0])\n",
    "\n",
    "print(modified_results_flattened)\n",
    "print(len(modified_results_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'super fast processer and really nice graphics card..', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Graphics', 'sentiment': 'POS'}, {'aspect': 'CPU', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}]}], [{'text': 'Oh my goodness-I am onot a happy camper.', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}, {'text': 'Oh my goodness-I am nto a happy camper.', 'span': [0, 0], 'aspect_sentiments': [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]}]]\n"
     ]
    }
   ],
   "source": [
    "# 4. map to lvl2 sentence packages\n",
    "# IMPORTANT: here I loose the last variants level because I do not actually need it\n",
    "# todo: loose levels --> only have one level per sentence when generating modified sentence packages\n",
    "\n",
    "modified_predictions = []\n",
    "modified_sentence = []\n",
    "check = 1\n",
    "for e, result in enumerate(modified_results_flattened):\n",
    "    i = package_indices[e]    \n",
    "    if i == check:\n",
    "        modified_sentence.append(result)\n",
    "    else:\n",
    "        modified_predictions.append(modified_sentence)\n",
    "        modified_sentence = []\n",
    "        modified_sentence.append(result)\n",
    "    check = i\n",
    "modified_predictions.append(modified_sentence)\n",
    "print(modified_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of results to check effectiveness of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_results = tp.compare_results(original_predictions, modified_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create adversarial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts, original_results, modified_texts, modified_results, successfull_modifications = tp.generate_results_lists(typo_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_dataset = pd.DataFrame(list(zip(original_texts, original_results, modified_texts, modified_results)),\n",
    "                 columns = ['original_sentence', 'original_prediction', 'modified_sentence', 'modified_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   original_sentence    1 non-null      object\n",
      " 1   original_prediction  1 non-null      object\n",
      " 2   modified_sentence    1 non-null      object\n",
      " 3   modified_prediction  1 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 160.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "adversarial_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>original_prediction</th>\n",
       "      <th>modified_sentence</th>\n",
       "      <th>modified_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh my goodness-I am not a happy camper.</td>\n",
       "      <td>[{'aspect': 'Laptop (general)', 'sentiment': '...</td>\n",
       "      <td>[Oh my goodness-I am onot a happy camper., Oh ...</td>\n",
       "      <td>[[{'aspect': 'Laptop (general)', 'sentiment': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         original_sentence  \\\n",
       "0  Oh my goodness-I am not a happy camper.   \n",
       "\n",
       "                                 original_prediction  \\\n",
       "0  [{'aspect': 'Laptop (general)', 'sentiment': '...   \n",
       "\n",
       "                                   modified_sentence  \\\n",
       "0  [Oh my goodness-I am onot a happy camper., Oh ...   \n",
       "\n",
       "                                 modified_prediction  \n",
       "0  [[{'aspect': 'Laptop (general)', 'sentiment': ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'adversarial_dataset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store adversarial_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_dataset.to_json(r'data/adversarial_dataset_multitypos.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Resluts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = tp.generate_results_df(original_sentences, modifyable_original_sentences, number_of_modified_sentences, successfull_modifications, pmethod='typos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perturbation Method</th>\n",
       "      <td>typos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tokenizer</th>\n",
       "      <td>en_core_web_sm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>en-laptops-absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>SemEval 2015 Laptops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total number of original sentences</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total number of modifyable original sentences</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total number of modified sentences</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total number of changed predictions through modification</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Success Rate</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0\n",
       "Perturbation Method                                                typos\n",
       "Tokenizer                                                 en_core_web_sm\n",
       "Model                                                    en-laptops-absa\n",
       "Dataset                                             SemEval 2015 Laptops\n",
       "Total number of original sentences                                    10\n",
       "Total number of modifyable original sentences                          2\n",
       "Total number of modified sentences                                     3\n",
       "Total number of changed predictions through mod...                     2\n",
       "Success Rate                                                    0.666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_sentence': 'GET THIS COMPUTER FOR PORTABILITY AND FAST PROCESSING!!!', 'original_result': [{'aspect': 'Portability', 'sentiment': 'POS'}, {'aspect': 'Performance', 'sentiment': 'POS'}], 'modified_sentences': [['GET THIS COMPUTER PORTABILITY AND FAST PROCESSING!!!', 'GET THIS COMPUTER FOR AND FAST PROCESSING!!!', 'GET THIS COMPUTER FOR PORTABILITY AND PROCESSING!!!', 'GET THIS COMPUTER FOR PORTABILITY AND FAST']], 'modified_results': [[[{'aspect': 'Portability', 'sentiment': 'POS'}], [{'aspect': 'Performance', 'sentiment': 'POS'}], [{'aspect': 'Portability', 'sentiment': 'POS'}], [{'aspect': 'Portability', 'sentiment': 'POS'}]]]}\n"
     ]
    }
   ],
   "source": [
    "print(loo_results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_map_loo = tp.generate_aspsent_map_dict(loo_results)\n",
    "aspects_map_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: ['Graphics', 'POS']},\n",
       " {1: ['CPU', 'POS']},\n",
       " {2: ['Performance', 'POS']},\n",
       " {3: ['Storage', 'POS']},\n",
       " {4: ['Portability', 'POS']},\n",
       " {5: ['Laptop (general)', 'POS']},\n",
       " {6: ['Price', 'POS']},\n",
       " {7: ['Laptop (general)', 'NEG']}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. aspect mapping: folgendermaßen:\n",
    "        i schaug, welche es gibt, schreib die in a Liste (so wie oben)(brauch nur asp_sent)\n",
    "2. mapp de auf nummern: mach a dict start bei 0\n",
    "\n",
    "3. hohl mir die o_results:\n",
    "    iterier drüber und kriag a liste mit aspects_sentiments\n",
    "    do iterier i nomol weil i de verdoppel, je nach dem wieviele modifications es für den satz geben hat\n",
    "    \n",
    "4. mach des geliche mit den modified results\n",
    "\n",
    "5. map de jeweils according zu der oanen map von #2\n",
    "\n",
    "\n",
    "hun 2 gleichlange listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}], [{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}]]\n",
      "[[{'aspect': 'Laptop (general)', 'sentiment': 'POS'}], [{'aspect': 'Laptop (general)', 'sentiment': 'POS'}]]\n"
     ]
    }
   ],
   "source": [
    "extended_original_results, extended_modified_results = tp.generate_multipredictions(original_results, modified_results)\n",
    "\n",
    "print(extended_original_results)\n",
    "print(extended_modified_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(dictionary.get, list_to_be_mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "asp_map = [{0: ['Graphics', 'POS']},\n",
    " {1: ['CPU', 'POS']},\n",
    " {2: ['Performance', 'POS']},\n",
    " {3: ['Storage', 'POS']},\n",
    " {4: ['Portability', 'POS']},\n",
    " {5: ['Laptop (general)', 'POS']},\n",
    " {6: ['Price', 'POS']},\n",
    " {7: ['Laptop (general)', 'NEG']}]\n",
    "extended_original_results = [[{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}], [{'aspect': 'Laptop (general)', 'sentiment': 'NEG'}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laptop (general)\n"
     ]
    }
   ],
   "source": [
    "aspects_sentiments = []\n",
    "for lst in extended_original_results:\n",
    "    for item in lst:\n",
    "        asp = item.get('aspect')\n",
    "print(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(extended_original_predictions, extended_modified_predictions)\n",
    "df_cm = pd.DataFrame(array, range(5), range(5))\n",
    "df_cm.index.name = 'original prediction'\n",
    "df_cm.columns.name = 'modified prediction'\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt=\"d\", linewidths=.1) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_typo = tp.generate_results_df(pmethod, ds, advds, extended_modified_predictions)\n",
    "results_typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " d_results.append(\n",
    "                    {\n",
    "                        'original_sentence': original_prediction['text'],\n",
    "                        'original_result': original_prediction['aspect_sentiments'],\n",
    "                        'modified_sentences': modified_sentences,\n",
    "                        'modified_results': modified_aspect_sentiments\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store results_typo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
