{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Word Detection\n",
    "\n",
    "1. Load Dataset \n",
    "2. pick 50 Documents for now\n",
    "3. make sentence splitting\n",
    "4. drop each word after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "import utils.text_processing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonline(filename, limit):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            counter += 1\n",
    "            py_obj = json.loads(line)\n",
    "            data.append(py_obj)\n",
    "            if counter > limit:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick first 60 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_jsonline(os.path.join('data', 'items_reviews_18.jl'), 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_id': 3321611, 'source_id': 277605655, 'title': 'This place is incredible!', 'text': 'I visited this b&b during a short trip to ride the famous Belgian pavé and it was perfect. The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle! It is in a really good location for riding or driving into Oudenaarde (approx 10 mins) and there are some brilliant restaurants close by. I cannot recommend this place enough!', 'user_rating': 5, 'lang': 'en', '_type': 'TripAdvisorHotelReviewItem'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Splitting\n",
    "- have list with text items\n",
    "- have list with splitted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Text form Reviews\n",
    "sentences = []\n",
    "for obj in data:\n",
    "    sentences.append(obj[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of single Sentences found in all available Text\n",
    "sentence_list = sent_tokenize(\". \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I visited this b&b during a short trip to ride the famous Belgian pavé and it was perfect.',\n",
       " 'The owners were really lovely people, the room was very comfortable and the breakfast was a delicious feast- ideal for big days out on the bicycle!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght sentences: 60 \n",
      " Length sentence_list: 435\n"
     ]
    }
   ],
   "source": [
    "print('Lenght sentences:', len(sentences),'\\n', 'Length sentence_list:', len(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input Reduction\n",
    "\n",
    "- Make a List with List with tokenized sentences\n",
    "- check length\n",
    "- go over one item (length) many times and remove item at index\n",
    "- append item to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Lists of tokenized sentences\n",
    "tok_sentences = []\n",
    "i = 0\n",
    "for sentence in sentence_list:\n",
    "    tok_sentences.append(sentence_list[i].split(' '))\n",
    "    i += 1\n",
    "\n",
    "len(tok_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'visited', 'this', 'b&b', 'during', 'a', 'short', 'trip', 'to', 'ride', 'the', 'famous', 'Belgian', 'pavé', 'and', 'it', 'was', 'perfect.'], ['The', 'owners', 'were', 'really', 'lovely', 'people,', 'the', 'room', 'was', 'very', 'comfortable', 'and', 'the', 'breakfast', 'was', 'a', 'delicious', 'feast-', 'ideal', 'for', 'big', 'days', 'out', 'on', 'the', 'bicycle!']]\n"
     ]
    }
   ],
   "source": [
    "print(tok_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tok_sentence):\n",
    "    sentence = ' '.join(tok_sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dropped_sentence_at_pos(sent,token):\n",
    "    tok_mod_sentence = sent.copy()    \n",
    "    tok_mod_sentence.pop(token)\n",
    "    return tok_mod_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the list of tokens in a sentence\n",
    "# and drop each word after the other\n",
    "# go over sentences in list of tokenized sentences\n",
    "sentence_packages = []\n",
    "for sent in range(len(tok_sentences)):\n",
    "    original_sentence = detokenize(tok_sentences[sent])\n",
    "    modified_sentences = []\n",
    "# go over token in sentence\n",
    "    for token in range(len(tok_sentences[sent])):\n",
    "        tok_mod_sentence = get_token_dropped_sentence_at_pos(tok_sentences[sent], token)\n",
    "        modified_sentences.append((tok_sentences[sent][token], detokenize(tok_mod_sentence)))\n",
    "    sentence_packages.append(\n",
    "        {\n",
    "            'original_sentence':original_sentence,\n",
    "            'modified_sentences':modified_sentences\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_sentence': 'All in all, a gem!', 'modified_sentences': [('All', 'in all, a gem!'), ('in', 'All all, a gem!'), ('all,', 'All in a gem!'), ('a', 'All in all, gem!'), ('gem!', 'All in all, a')]}\n"
     ]
    }
   ],
   "source": [
    "print(sentence_packages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juhuuuuu :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict with BERT for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105879"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import utils.text_processing as tp\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model.eval();\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29155, 10228, 12548, 10320, 10855, 136]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perfect.', 'bicycle!', 'some', 'cannot', 'in', 'expect', 'fantastic', 'superb', \"couldn't\", 'have', 'Materke.', 'riding.', 'perfect', 'again!.', 'as', 'already', 'very', 'options.', 'safe', '(and', 'enough', 'confusion', 'all', 'impossible...they', 'late.', 'city.', 'busy', 'ideal.', 'worth', 'Preto.', 'bathrooms', 'Great', 'Staff', '&', 'work.', 'eggs.', 'Wonderful', 'wait', 'star', 'fabulous', 'included.', 'welcome,', 'and', 'damage.', 'floors.', 'bed.', 'garden.', 'no', 'gratefully.', 'if', 'wanted', 'castle.', 'friendly', 'coffee.', 'again', 'wanted', 'castle.', 'friendly', 'coffee.', 'again', 'and', 'process', 'woman', 'and', 'excellent', 'recommend', 'high', 'offer.', 'but', 'although', 'serviceable.', 'bit', 'never', 'slightly', 'find.', 'located.', 'and', 'disappointed!!', 'rooms!!', 'and', 'comfortable', 'Stayed', 'hard', 'so', 'here.', 'and', 'grounds', 'of', 'Basically', 'discordant', 'average.', 'disappointing', 'comfortable,friendly', 'beautiful.', 'special.', 'very', 'this', 'Difficulty', 'extra', 'reflecting', 'otherwise', 'was', 'person.', 'restaurant.', 'but', 'on', 'moss', 'ordinary.', 'Then', 'irritated', 'love', 'done.', 'chicken..', 'only', 'went', 'not', 'speechless.', 'less.', 'review..annoyed', 'not', 'me', 'room', 'quite', 'always,', 'need', 'No', 'No', 'amazing..', 'stream.', 'with', 'enough', 'slippers.', 'because', 'but', 'just', 'long.', 'mask.', 'the', 'to', 'centuries.', 'Conques.', 'efficient.', 'gardens.', 'impeccably', 'day.', 'but', 'had.', 'spite', 'as', 'helping', 'fresh.', 'delicious.', 'return', 'friends.', 'staff.', 'from', 'quiet.', 'excellent', 'Reservation', 'appreciated.', 'thing.', 'helpful.', 'excellent', 'excellent', 'Rhein.', 'or', 'meal.', 'keep', 'staff', 'were', 'like', 'very', 'implies,', 'blessing', 'good,', 'putting.', 'issue.', 'internet.', 'throughout!', '.', 'a', 'All', 'hoping', 'However', 'tiny.', 'exactly', 'tried', 'but', 'welcome.', 'never', 'told', 'should', 'spare', 'down', 'On', 'might', '.', 'seaside.', 'helpful.', 'included.', 'beach..', 'all..', 'The', 'clean.', 'view.', 'view.', 'great', 'some', 'gem....', 'best', 'clean.', 'elsewhere.', 'owner', 'hotel!!?)', 'But', 'away', 'quits!', 'accept', 'etc.', 'abusive', 'horrendous.', 'Not', 'not', 'makes', 'aggravate', 'Century.', 'Great', 'Lovely', 'bonus', 'room.', 'forgot', 'photos.', 'little', 'great', 'tub.', 'village.', 'for', 'better', 'spacious.', 'ok.', 'dog.', 'typical', 'superb', 'beautiful', 'gorgeous', 'comfy.', 'excellent', 'breakfast.', 'nearby.', 'difficult', 'had', 'longer.', '.', 'Verde.', 'welcoming.', 'authentic', 'wholly', 'perfectly', 'morning.', 'well', 'day.', 'real', 'island.', 'it!', 'Nice', 'but', '.', 'Verde.', 'slightly', 'so', 'rooms', \"wasn't\", 'perfectly', 'available!', 'however', 'lovely', 'street.', 'upstairs.', 'delicious!', 'daughter!', 'recommend', 'complementary.', 'be.', 'LOL!', 'really', 'money.', 'problem..', 'considering', 'conditioner.', 'parking', 'super', 'Spain', 'location.', 'staff.', 'e.g.', 'not', 'comfort.', 'away', 'some', 'noise.', '4', 'us', 'clean.', 'if', 'old', '4-star', 'not', 'country.', 'in', 'amazing.', 'atmosphere,', 'perfect.', 'city.', 'well', 'clean.', 'Good', 'good', 'Staff', 'and', 'No', 'efficient.', 'modern.', 'bars', '.', 'This', '...', 'stay!.', 'central.', 'odd', 'seemed', 'hostel.', 'it', \"didn't\", 'odd', 'slow', 'not', '(except', 'value', 'all', 'Not', 'do.', 'hostels.', 'Selina.', 'refund.', 'in', 'delicious!', 'more', \"Can't\", 'Jorge!', 'Joey.', 'Ideally', 'dodgy.', 'fantastic.', 'need.', 'average.', 'okay.', 'annoying.', 'patient..', 'cheery.', 'sale.', 'room.', 'Would', 'safe', '.', 'stayed', 'very', 'got', 'nice', 'drink!', 'Overall,', 'clean,', 'view.', 'decent', 'breakfast', 'center.', 'the', 'hotel.', 'etc.)', 'dinner.', 'Ouro', 'reservations', 'chamber', 'were', 'seemed', 'rooms.', 'arrived.', '(party', '(with', 'was', 'minimal', 'with', 'view).', 'somewhat', 'and', 'but', 'few', 'limited', 'CNN).', 'bath.', 'good,', 'room.', 'not', 'is', 'lovely', 'juices.', 'Scrambled', 'satisfying.', 'fabulous', 'view.', 'hill,', 'we', 'nor', 'disturbed', 'silent.', 'occasionally', 'plump.', 'cleanliness.', 'value-priced.', 'service', 'comfort!', 'unacceptable', 'hotel!', 'delicious.', 'but', 'PM).', 'pool', 'nothing', '20', 'little', 'quiet.', 'but', 'when', '(that', 'might', 'gem!']\n"
     ]
    }
   ],
   "source": [
    "important_words = []\n",
    "\n",
    "for package in sentence_packages:\n",
    "    original_sentence = package['original_sentence']\n",
    "    # print('new package: ' + original_sentence)\n",
    "\n",
    "    original_result = tp.predict_sentiment(model, tokenizer, original_sentence)\n",
    "    highest_relative = 0\n",
    "    highest_relative_word = None\n",
    "\n",
    "    for item in package['modified_sentences']:\n",
    "        word = item[0]\n",
    "        sentence = item[1]\n",
    "        modified_result = tp.predict_sentiment(model, tokenizer, sentence)\n",
    "        relative = abs(original_result - modified_result)\n",
    "# OBACHT >=\n",
    "        if relative >= highest_relative:\n",
    "            highest_relative = relative\n",
    "            highest_relative_word = word\n",
    "            \n",
    "    important_words.append(highest_relative_word)\n",
    "\n",
    "assert(len(important_words)==len(sentence_packages))    \n",
    "print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'important_words' (list)\n",
      "Stored 'sentence_packages' (list)\n"
     ]
    }
   ],
   "source": [
    "%store important_words\n",
    "%store sentence_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
