\section{Introduction}
% <The introduction includes the motivation for writing the Master's Thesis and a detailed definition of the problem.>

With the integration of computer vision for self-driving cars, natural language processing for controlling our smart homes as well as other applications of machine learning in our daily lives, our reliance on technology increases and with it also the concerns about security of machine learning. These security concerns grow as natural language processing models are deployed in production systems such as fake news detectors and home assistants. Adversarial examples are small, and often imperceptible perturbations applied to the input data in an effort to fool a deep classifier to incorrect classification.
Adversarial examples are a way to highlight model vulnerabilities and are useful for evaluation and interpretation. 
In my Master's Thesis I want to generate adversarial text to attack a model based on the Bidirectional Encoder Representations from Transformers (BERT) used for Aspect-Based Sentiment Analysis (ABSA), a task in Natural Language Processing (NLP). NLP is a field of artificial intelligence (AI) that addresses the interaction between computers and humans using natural language. In 2018 Google achieved a breakthrough in NLP by introducing BERT, a powerful neural network architecture that leverages transfer-learning by pre-training the model on a large corpora with subsequent fine-tuning on a variety of downstream-tasks like text-classification, question answering, sentiment analysis and more. 
Given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection it is highly concerning that the security vulnerabilities of Deep Learning models in NLP are still largely unknown and not a large field of research.

\section{Theoretical Background}
Deep neural networks (DNNs) are neural networks that are inspired by the biological neural network of a human brain to learn from examples and build knowledge. 
The strengths of DNNs apart from their high performance is that their architecture can be adapted to model various modalities like image, text, video, and audio.
Their architecture is organized as a series of layers of neurons. Each neuron serves as an individual computation unit, which is linked through connections of different weights and biases and transmits the results of its activation function on its inputs to the neurons of the next layer \cite{zhang2019adversarial}.
In recent years DNNs have gained significant popularity in many AI applications such as computer vision (CV), natural language processing, and many more.

\subsection{Natural Language Processing}
In NLP, researchers aim to explore how computers can be used to understand natural language. Fundamental applications of NLP include automatic summarization, machine translation, language identification, Part-of-Speech (POS) tagging, question answering, textual entailment\footnote{A binary prediction of a relation between two text fragments. E.g. Peter is snoring - A man sleeps.}, text classification, and sentiment analysis (SA). SA is an active field of research in NLP and aims to detect opinions in text. In contrast to SA, ABSA is a more fine-grained task that extracts both the aspect mentioned and the sentiment associated with this aspect \cite{pavlopoulos2014aspect}.

\subsection{BERT}
In 2018, Google achieved a breakthrough in NLP by introducing the Bidirectional Encoder Representations from Transformers \cite{devlin2018bert}. BERT is a stack of transformer encoder layers that consist of fully-connected neural networks, so-called heads. For every input token in a sequence, e.g., a sentence, each head computes key, value, and query vectors, which are then used to create a weighted representation. The outputs of all heads in the same layer are combined and run through a fully-connected layer (see Figure \ref{fig:BERT}).
\begin{figure}
    \centering
    \includegraphics[ width = .8\textwidth]{img/BERTfine-tuning.png}
    \caption{BERT fine-tuning \cite{devlin2018bert}.}
    \label{fig:BERT}
\end{figure}
The BERT workflow conventionally consists of (1) pre-training and (2) fine-tuning. Pre-training uses two self-supervised tasks: masked language modeling and next sentence prediction. In fine-tuning for downstream applications, one or more fully-connected layers are added on top of the final encoder layer. To compute the input-encoding, BERT first tokenizes the given sentences into referring word pieces and then combines three embedding layers (token, position, and segment) to obtain a fixed-length vector.

\subsubsection{Token Embedding Layer}
The purpose of the token embedding layer is to tokenize the input (e.g. "I like strawberries") and transform it into a vector representation of fixed dimensions (see Fig \ref{fig:BERT_tokenEmbedding}). In case of BERT base, each word is represented as a 768-dimensional vector.
\begin{figure}
    \centering
    \includegraphics[ width = .5\textwidth]{img/BERTtoken_embedding.jpg}
    \caption{BERTbase Token Embedding Layer.}
    \label{fig:BERT_tokenEmbedding}
\end{figure}
Special tokens are needed for sentence classification. [CLS] is added in front of every input example, and [SEP] serves as a separator token (e.g. separating questions/answers).
In order to achieve a balance between vocabulary size and out-of-vocab words a tokanization method, called WordPiece is used which in this case splits the word "strawberries" into "straw" and "berries". A detailed description of this method can be found in section 4.1 in \cite{wu2016google}.  Through the use of WordPiece tokenization the BERT vocabulary size can be held to a minimum of 30,522 words or word pieces. 
\subsubsection{Segment Embedding Layer}
The purpose of the segment embedding layer is to enable BERT to distinguish inputs in a given sentence pair. Sentence pairs are used for example for the task of question answering. Suppose the input sentence pair is "I like strawberries." and "I like ice-cream.". The segment embedding layer uses two indices 0 and 1 as vector representations to assign tokens to their corresponding input segment (see Fig \ref{fig:BERT_segmentEmbedding}).

\begin{figure}
    \centering
    \includegraphics[ width = .5\textwidth]{img/BERTsegment_embedding.jpg}
    \caption{BERTbase Segment Embedding Layer.}
    \label{fig:BERT_segmentEmbedding}
\end{figure}

\subsubsection{Position Embedding Layer}
The position embedding layer leverages the encoding of positional information in an input sequence inside a Transformer. BERT processes input sequences up to a length of 512. Therefore the position embedding layer is a lookup table of size 512x768 where the each row is a vector representation of the word in the corresponding position. In an input like "I like you" and "You like me", both "I" and "You" will have identical position embeddings since they are the fist token in the input sequence. 

The two versions of BERT (base and large), differ in the number of layers, their hidden size, and the number of attention heads.  

\subsection{Adversarial Examples}

In 2013, \cite{szegedy2013intriguing}  were the first to discover the vulnerability of several machine learning models through adversarial examples: inputs crafted by adversaries with the intent of causing deep
neural networks to misclassify \cite{papernot2016crafting}. These carefully curated examples are correctly classified by a human observer but can fool a target model, raising serious concerns regarding the security of existing machine learning algorithms. 
The altered input data is crafted from a valid sample and either the model's gradients or its output data. 
To describe adversarial examples in a formalized manner, a classifier can be defined as a function \(C(x) = y\). This function has the input value \(x\) and the output value \(y\). After an attacker modifies the input value \(x\), it will become \(x'\), which is incorrectly classified by the algorithm and leads to \(C(x) \neq C(x')\). 
The difference between \(x\) and \(x'\) is commonly called the distance matrix \cite{carlini2017towards}. 
See Figure \ref{fig:advEx} for an example of an applied adversarial perturbation. The image of a panda, which is originally classified correctly with the label "panda" will be misclassified as "gibbon" after adding imperceptible noises. 
\begin{figure}
    \centering
    \includegraphics[ width = .8\textwidth]{img/advExample.jpg}
    \caption{Adversarial example in computer vision \cite{papernot2016crafting}.
    Original image of a panda bear shown on the lift side. The right side shows the misclassified image with the added perturbation, shown in the middle.}
    \label{fig:advEx}
\end{figure}

The notation "adversarial example" is used in follow-up research to denote all kinds of perturbation samples in a general manner. In further research, \cite{goodfellow2014explaining} found that a wide variety of models with different architectures misclassify the same adversarial example, even when trained on different subsets of training data. Those examples are called transferable adversarial examples. These findings indicate that adversarial examples pose a fundamental blind spot in machine learning algorithms.

In research, we distinguish between two types of settings.
An attacker operates either in a black or white-box setting, depending on the degree of access, he or she has to the system. While in a black-box setting, the attacker does not have access to the target model's internal architecture or its parameters, he or she has full access to the target model, its parameters, and input feature representations in the white-box setting. The adversary cannot alter the training data or the target model itself in both cases.

Other works have designed attacks in the "extended white-box" or "gray-box" setting, which describes the scenario of some aspects of the setup being known to the attacker while some are not \cite{vivek2018gray}. However, the definitions for those scenarios seem to vary in the different approaches.

Depending on the purpose of the adversarial attacks, they can be categorized as targeted and non-targeted attacks. While in a targeted attack, the output category of a generated example is intentionally controlled to a specific target category, a non-targeted attack does not take the category of misclassified results into consideration \cite{vijayaraghavan2019generating}.

\subsection{Adversarial Examples in Natural Language Processing}
In recent years the research efforts to generate adversarial examples for DNNs for textual applications increased. \cite{Jia2017AdversarialEF} were the first to conduct adversarial attacks on textual DNNs and gained attention in the NLP community. 
The mapping from natural language into a corresponding vector is done through the creation of word embeddings. The three main methods to create those word embeddings are word-count based encoding (bag-of-words method), one-hot encoding, and dense encoding.
Using gradient-based adversarial methods used in CV, in the text domain, can result in altered semantics, syntactically-incorrect sentences, or invalid words that cannot be matched with any words in the word embedding space. When attacking textual DNNs, it is crucial to carefully design variants or distance measures of the perturbation \cite{zhang2019adversarial}.
In an image, each pixel has a number representation within a fixed range.  A common representation is to use floating numbers in the range [0,1] or {0,1,...,255}. This indicates that the numerical representation of a pixel gives an insight into the characteristics of the image, and from this, it can be deduced that pixels with similar numerical representations are closely related in terms of their characteristics.
The case is different with the numerical representation of word tokens. 
Thus, the numerical representation of the word 'cat' may be close to that of the word 'car'.
However, that does not mean that the semantics of the words are similar, as is the case with pixels. The generation of adversarial examples in the text domain is therefore considered more challenging\cite{carlini2018audio} since it is not possible to craft \(x'\) through gradient calculations. 
\cite{liang2017deep} present an effective method to craft adversarial text samples by first identifying the text items essential for classification by computing the cost gradient of the input (white-box attack) as well as generating a series of occluded test samples (black-box attack). They were able to successfully fool character-level and word-level DNN-based text classifiers through the conduction of three perturbation strategies, namely insertion, modification, and removal. 
\cite{gao2018black} use a scoring strategy to produce character-level transformations in the black-box setting on RNN based natural language classifiers. They conduct small edit operations to a text sequence such that a human would consider it similar to the original sequence. They do this by first targeting the important tokens in the sequence and then execute a modification on those tokens that can force the classifier to make a wrong prediction. To create small edit distances, they use the Levenshtein distance, a metric to measure the similarity between sequences, and perturb using four methods, namely substitution of a letter in a word with a random letter, deletion of a random letter, insertion of a random letter, and swapping two adjacent letters. Their method achieves better results than FGSM  which is most likely because the selection of words is more important than the alteration of words. 
\cite{jin2019bert} were the first to attack the pre-trained BERT model. They first conduct a word importance ranking by analyzing the prediction change before and after deleting one word. After ranking the words according to their importance, they filter out stop words to avoid grammar destruction. Their perturbation approach is word-level based, as they replace the original word with synonyms and use word embedding vectors to measure how well different models judge the semantic similarity between the words. Also, they check part-of-speech (POS) to assure the maintenance of grammar and calculate the cosine similarity score to measure semantic similarity. \cite{goodman2020fastwordbug} generate small utility-preserving text perturbations in a black-box setting by using a scoring method similar to \cite{gao2018black} and \cite{jin2019bert} to identify important words that affect text classification. They find a relationship between POS and the importance of words for the influence of the prediction. 

\section{Objectives and Research Question}
% <Within the objective and research question, the author describes the focus of the Master's Thesis as well as the deduction of the appropriate research question(s).>

In my study, I want to focus on the field of ABSA and target the state-of-the-art BERT base model as well as a BERTbased ABSA Deep Learning model of the company DeepOpinion\footnote{https://deepopinion.ai/}. The thesis is supposed to determine the robustness of the BERT based NLP model for the task of ABSA against input level adversarial examples.
Additionally, I want to investigate whether the generated adversarial examples can be transferred between models with a different degree of fine-tuning. A defense strategy against adversarial attacks is adversarial training. By training the models on the generated adversarial dataset, I want to gain knowledge about whether I can improve robustness against adversarial attacks and, as a result, improve the overall model performance.

The resulting research questions answered in my thesis will be the following:

How robust is BERT based NLP for the task of ABSA against input level adversarial examples?

(a) How does fine-tuning influence robustness against adversarial attacks?

(b) How does adversarial training influence model performance?

(c) How does adversarial training influence model robustness against adversarial attacks?

\section{Methodology}
In this thesis, I will primarily investigate the prominent pre-trained BERT model introduced by Google in 2018.
I want to determine what influence the degree of fine-tuning on a target domain has on the robustness of NLP models against adversarial attacks. Fine-tuning is a concept of transfer learning, where the world knowledge gained when training a task is transferred to a related task or domain \cite{pan2009survey}. To do so, I will investigate the fine-tuned hotel-model of the company DeepOpinion.
In the context of this thesis, I will assess the degree of fine-tuning as the difference in performance a model was able to achieve before, and after the fine-tuning process. 
To obtain knowledge about the transferability of the generated adversarial examples, I will compare the effect of different degrees of fine-tuning on robustness against the attacks. 



In my thesis, I will conduct non-targeted attacks in the white-box setting, where I assume that the adversary possesses knowledge about the probabilities of the predictions. Since there is more realistic relevance for this scenario, non-targeted attacks will be executed.

In the typical white-box scenario of generating adversarial attacks, gradient calculations are used to determine which features have to be modified in order to generate an adversarial example. The method mainly derived from the fast gradient sign method (FGSM) and is very effective in computer vision \cite{goodfellow2014explaining}. However, as mentioned above, this cannot be applied to text directly since perturbed word embeddings do not necessarily map to valid words. Other approaches are optimization-based and model-based attacks, which treat adversarial attacks as an optimization problem where the constraints are to maximize the loss of target classifiers and to minimize the difference between original and adversarial examples. While the optimization-based approach uses optimization algorithms directly, the model-based approach trains a separate model to generate adversarial examples \cite{xu2020elephant}. Successful attacks in text were conducted in the white-box setting with HotFlip \cite{ebrahimi2017hotflip} and in the black-box setting with DeepWordBug \cite{gao2018black} and TextBugger \cite{li2018textbugger}. 

\cite{papernot2016crafting} used the gradient-based approach for crafting adversarial input sequences for recurrent neural network-based text classifiers and chose the word at a random position in a text input to replace it with it's nearest word embedding vector. Since there is no guarantee that words close in the embedding space are semantically similar, this approach may replace words with irrelevant, semantically different others. 
In order to generate semantically meaningful adversarial examples, \cite{samanta2017towards} used saliency maps. This approach, however, is difficult to perform automatically. In their paper, \cite{gao2018black} introduced a two-step approach to generate adversarial attacks in text. Unlike \cite{papernot2016crafting}, \cite{gao2018black} deviate the process of generating adversarial text into two main steps. They first determine which words are critical for the model's prediction by introducing a word importance scoring function. In a second step, they generate adversarial text by making imperceptible edit operations to a text sequence, which forces the classifier to make wrong predictions.

In my thesis, I want to make use of their two-step approach, including important word detection. That way, I want to determine the word which has a critical influence on the model's prediction in order to maximize the loss of the target classifiers and minimize the input modification effort as well as the difference between original and adversarial examples. 
To rank words by importance for the prediction, its effect on the output classification has to be measured. As already explained, it is difficult to measure a word's perturbation by calculating gradients viably. 
For that reason, I will make use of \cite{jin2019bert}'s method of removing each word of a sentence one by one and let the model predict the incomplete sentences. 
 Comparing the prediction before and after a word is removed reflects how the word influences the classification result. This procedure allows me to enhance the efficiency of my attacks.

To execute the perturbations, I will focus on the input level rather than the embedding or semantic level. The examples will be generated on a character level basis and can be subdivided into three steps:
(i) Investigation of the influence of additional or missing punctuation marks,
(ii) with the aid of the Birkbeck file, a list of common typos documented by the Oxford Text Archive, the robustness of the models on misspellings will be tested. The file contains 36,133 misspellings of 6,136 words.
(iii) By replacing letters with visually similar-looking symbols, e.g., @ for a or 0 for o, the effect of 'Leet Speech' on the different NLP models will be explored.
Perturbations (ii) and (iii) will be applied to the beforehand detected important words.

The attacks will be executed on the target models individually.
The result will be datasets with generated adversarial examples. To test the transferability, I will predict the datasets with the respective other models. The accuracy of the predictions will provide information about whether and to what extent the examples are transferable.
The final part of my work will be adversarial training. 
The first step here will be the modification of a different dataset which the models were not trained on before and modify it in order to create adversarial examples. After that I will use the generated adversarial examples of the original dataset as a training set for both models and explore, using the unseen adversarial examples, whether adversarial training influences the robustness against these attacks. Additionally, I will predict a regular dataset and investigate whether adversarial training has improved the overall model performance.

\section{Conclusions, Limitations and Further Research}
% <Formulation of the expected results and demonstration of the limitations the thesis will have (e.g. due to the limited time frame the thesis has). Over and above aspects of further research should be drawn.>

The idea is to implement the experiments in the running BERTbased ABSA Deep Learning Hotel Model of the company DeepOpinion. The model is already fine-tuned on user-generated reviews and trained on the ABSA text classification task. This gives me a head start, which limits the scope of my work. However, the model being the company's core technology, cannot be published in my thesis, which limits the reproducibility of my work.

\section{References}
\bibliography{References}

\section*{Road Map}

\begin{tabular}{ |p{1.5cm}||p{7cm}|p{2cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Roadmap} \\
 \hline
Number & Task & Duration in day(s) & Date\\
 \hline
 1 & Model Trianing on ABSA Task & 2 & 10 mar 2020 \\
 2 & Important Word Detection & 3 & 15 mar 2020\\
 3 & Generation of Adversarial Attacks & 5 & 15 may 2020\\
 4 & Generation of adversarial dataset & 2 & 20 may 2020\\
 5 & Execution of Perturbations & 1 & 22 may 2020\\
 6 & Evaluation of Results & 7 & 23 may 2020\\
 7 & Transferability Testing & 2 & 1 jun 2020\\
 8 & Adversarial training & 3 & 5 jun 2020 \\
 \hline
\end{tabular}

% <Presenting a realistic time schedule in regards to the Master's Thesis.>


